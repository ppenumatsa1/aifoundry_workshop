# Evaluate the performance of your application in the Azure AI Foundry

This exercise will guide you through exploring Manual and Automatic evaluations to assess and compare the performance of your AI applications using the Azure AI Foundry portal. 

## Prerequisites
- Create an AI hub and project in the Azure AI Foundry
- Deploy a LLM/SLM model

## Manually evaluate your application in the Azure AI Foundry portal Playground


- Go to the Azure AI Foundry project you have created. 

    ![alt text](../images/12_image.png)

- Then Click on *"Playground"* option as shown below. Select Chat Playground. 

    ![alt text](../images/12_image1.png)

- You can select deployed model , you can change parameters like Past messages, Temprature, Top P etc. 

- Follow instructions in section 1.2 and 1.3 to manually evaluate your RAG application using prompt engineering and add your data/index. 

- Refer system prompt and prompt engineering guidance provided in section 1.2 to try and compare responses from  different models by comparing their responses to specific prompts relevant to your use case. 

  Refer below screenshot for details: 

    ![alt text](../images/12_image-2.png)
   

## Use Manual Evaluation option to evaluate more than one prompts

You can use Manual evaluation option to feed more than one prompts at a time to the LLM (or RAG pattern) and get the results in bulk mode. 

1. In the **Chat playground**, select the **Evaluate** dropdown from the top bar, and choose **Manual evaluation**.
2. You can change the default **System message** to specific system message for your use case.
3. Make sure to change the Configuration pointing to the model and change parameters. 
4. Click on *Add your data* tab and select "aiworkshop-benefits-integratedvindex" you have created in section 1.3 
5. In the **Manual evaluation result** section, add the following five questions as separate **Inputs**:

   - Input: What is the annual deductible for the Northwind Standard plan?
   - Expected response: The deductible for the Northwind Standard plan is $2,000 per person, per year.

   - Input: What is the copayment in Northwind Standard plan for a specialist visit?
   - Expected response: The copayment for a specialist visit under the Northwind Standard plan is $50

   - Input: Are preventive care services covered?
   - Expected response: Yes, preventive care services are covered at 100% under the Northwind Standard plan

   - Input: Does Perksplus cover horseback riding?
   - Expected response: Yes, Perksplus covers horsebackriding

   - Input: Do we have holiday on President's day?
   - Expected response: Yes, President's Day is a holiday according to the Contoso Electronics Holidays Calendar for 2025. It falls on February 17

*Alternatively, you can also upload csv or jsonl file with these columns and import this data using **Improt test data** option*  
   
6. Select **Run** from the top bar to generate outputs for all questions.
![alt text](../images/14_image.png)
7. Manually review the outputs for each question by selecting the thumbs up or down icon at the bottom right of a response. 
8. Based on your review/feedback you will see score on the screen. 
![alt text](../images/14_image-1.png)
8. Select **Save results** from the top bar. Enter `manual_evaluation_results` as the name for the results.
7. Navigate to **Evaluation** using the left menu.
8. Select the **Manual evaluations** tab to find the manual evaluations you just saved.
![alt text](../images/14_image-2.png)

## Evaluate your Application with Automatic Evaluations

You can use Automated evaluations and generate metrics for each data row in your test datasets.

You can choose one or more evaluation metrics to assess the output from different aspects. 

You can create an evaluation run from the evaluation, model catalog or prompt flow pages in Azure AI Foundry portal. 

Then an evaluation creation wizard appears to guide you through the process of setting up an evaluation run.



1. Select the **Automated evaluations** tab and create a **New evaluation** :

   ![alt text](../images/14_image-3.png)

2. Select below *Model and prompt* option on the **What do you want to evaluate?** popup. 
3. Refer below screenshot for details about
   - **Evaluation name**: *Enter a unique name*
   - **Deployment**: *Choose model you have deployed for evaluation*
   - Set up parameters or keep them as default. 
   - Update System prompt 
   - For **Select the data you want to evaluate**: Add your dataset
   - Use any test datasets (.jsonl files) available in the data folder and upload it to the UI.
   - Once you upload file, you will see top 3 rows/prompts gets loaded on the screen as shown below: 
   ![alt text](../images/14_image-4.png)
   - Make sure data column mapping is set as below:
   ![alt text](../images/14_image-6.png)

4. Choose all 3 type of evaluations as shown below: 

   ![alt text](../images/14_image-5.png)

   *Note: Risk/Safety evaluations are in preview may not be available in only few regions at the time of creating these instructions. So please check if it is available in your region* 

6. Make sure you have selected GPT 4/4o or 4o-mini models or any other supported models as a judge. 
Not all models are supported and used as Judge. Please refer latest documentation (added in references) for details.

7. Click **Create**, to create and submit your evaluations.
8. Wait for the evaluations to complete, refreshing if necessary. This step may require few minutes to execute as required compute is created and evaluations are executed on that compute. At the end of successful evaluation completion, you will be able to access all the metrics on *Report* tab and details of evaluations on *data* tab. 

   ![alt text](../images/14_image-7.png)

9. Explore the **Metric dashboard** and **Data** for more details.

10. You can also export results using *Export Result" option. 


### <span style="color:Yellow"> References: 

https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-approach-gen-ai

https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-prompts-playground

https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-generative-ai-app

</span>
